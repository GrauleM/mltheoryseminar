# Harvard CS 229br: Advanced Topics in the theory of machine learning

__Boaz Barak__

__Mondays 12-3__


See [home page for Harvard CS 229r and MIT 18.408](/).


__Course description:__
This will be a graduate level course on recent advances and open questions in the theory of machine learning and specifically deep learning. We will review both classical results as well as recent papers in areas including classifiers and generalization gaps, representation learning, generative models, adversarial robustness and out of distribution performance, and more. 

This is a fast-moving area and it will be a fast-moving course. We will aim to cover both state-of-art results, as well as the intellectual foundations for them, and have a substantive discussion on both the “big picture” and technical details of the papers. In addition to the theoretical lectures, the course will involve a programming component aiming to get students to the point where they can both reproduce results from papers and work on their own research. This component will be largely self-directed and we expect students to be proficient in Python and in picking up technologies and libraries on their own (aka “Stack Overflow oriented programming”). We will ensure students have access to the appropriate computational resources (i.e., GPUs).

__MIT "Sister seminar":__ This Harvard seminar will be coordinated with a "sister seminar" at MIT, taught by Ankur Moitra. We recommend that students taking CS 229br also take the MIT course, but this is not required. The two courses will share some but not all lectures and assignments.  So, if you take CS 229br, please keep the Wednesday 12-3 slot free as well. 
 

### Tentative list of topics (subject to change): 


* Generalization bounds, and the extent that they are meaningful and/or proxies to actual performance.
* Out of distribution performance, Imagenet v2 and CIFAR v2, domain shift
* Unsupervised and self-supervised learning
* Transfer learning, meta learning
* Generative models (GANs,  VAE variants, DDPM, autoregressive models,...) 
* Natural language processing, language models
* Theoretical neuroscience, interpretation and visualization of neural networks
* Reinforcement learning, bandits
* Control theory
* Privacy, adversarial robustness, fairness/calibration
* Statistical physics view of machine learning.

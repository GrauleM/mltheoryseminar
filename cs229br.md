# Harvard CS 229br: Advanced Topics in the theory of machine learning

__Boaz Barak__

__Mondays 12-3__

__Unofficial teaching fellows:__  [Yamini Bansal](https://yaminibansal.com/) ybansal (at) g.harvard.edu  [Gal Kaplun](https://www.galkaplun.com/) galkaplun (at) g.harvard.edu [Dimitris Kalimeris](https://www.dkalimeris.com/) kalimeris   (at) g.harvard.edu   [Preetum Nakkiran](https://preetum.nakkiran.org/) preetum (at) cs.harvard.edu 


See [home page for Harvard CS 229r and MIT 18.408](https://boazbk.github.io/mltheoryseminar/).

Introductory blog post by Boaz: [Machine Learning Theory with Bad Drawings](https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/)

__Course description:__
This will be a graduate level course on recent advances and open questions in the theory of machine learning and specifically deep learning. We will review both classical results as well as recent papers in areas including classifiers and generalization gaps, representation learning, generative models, adversarial robustness and out of distribution performance, and more. 

This is a fast-moving area and it will be a fast-moving course. We will aim to cover both state-of-art results, as well as the intellectual foundations for them, and have a substantive discussion on both the “big picture” and technical details of the papers. In addition to the theoretical lectures, the course will involve a programming component aiming to get students to the point where they can both reproduce results from papers and work on their own research. This component will be largely self-directed and we expect students to be proficient in Python and in picking up technologies and libraries on their own (aka “Stack Overflow oriented programming”). We will ensure students have access to the appropriate computational resources (i.e., GPUs).

__MIT "Sister seminar":__ This Harvard seminar will be coordinated with a "sister seminar" at MIT, taught by Ankur Moitra. We recommend that students taking CS 229br also take the MIT course, but this is not required. The two courses will share some but not all lectures and assignments.  So, if you take CS 229br, please keep the Wednesday 12-3 slot free as well. 
 

### Tentative list of topics (subject to change): 


* Generalization bounds, and the extent that they are meaningful and/or proxies to actual performance.
* Out of distribution performance, Imagenet v2 and CIFAR v2, domain shift
* Unsupervised and self-supervised learning
* Transfer learning, meta learning
* Generative models (GANs,  VAE variants, DDPM, autoregressive models,...) 
* Natural language processing, language models
* Theoretical neuroscience, interpretation and visualization of neural networks
* Reinforcement learning, bandits
* Control theory
* Privacy, adversarial robustness, fairness/calibration
* Statistical physics view of machine learning.

### FAQ

_Is there a complete plan of all lectures and assignments?_ 

No - this course will be an experiment, for both me and the students, and we will figure out how much we can cover and in what way as we go along. The goal is to start with some of the foundations and to get quickly to talk about recent papers. The intention is that students  will get to the point where they can read (and sometimes also can reproduce) recent ML papers, and hopefully also be able to generate new insights.

_What will the format of the course be like?_ 

We will have weekly lectures/discussions, and experimental homeworks/projects. The lectures will focus on describing and discussing papers and theory, while problem sets / projects will be more empirical. We will have formal or informal "sections" where the unofficial TFs will help out in technical issues with implementations, but we will also rely on students looking up material and helping one another.

_What is expected out of students?_

Students will be expected to do some reading before lectures, and to work on some experimental homework assignments, typically involving reproducing a paper, or trying out some experiment. The lecture will not discuss how to run experiments or implement neural networks, but the teaching fellows will be available.  We will also expect students to look up resources on their own (such as this [excellent deep learning course of LeCun &  Canziani](https://atcold.github.io/pytorch-Deep-Learning/)) and to help one another. There will also be some project, and students might potentially also need to write scribe notes for one lecture.

_How will students be graded?_ 

The course is intended for graduate students or advanced undergraduate students who have mostly completed their requirements but are deeply interested in the material for its own sake. The method of grading will be decided later on. At the moment we have several "unofficial TFs" that are spending effort in designing assignments that will get you better at being able to run your own experiments, but we don't have any official TFs. We will try to find ways that you can get feedback on your work, even if we don't have the resources to grade it. 


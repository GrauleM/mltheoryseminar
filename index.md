
__Spring 2021__ 

__Harvard CS 229br: [Boaz Barak](https://boazbarak.org)  Mondays 12-3pm Eastern__ 


__MIT 18.400: [Ankur Moitra](http://people.csail.mit.edu/moitra/)  Wednesdays 12pm-3pm Eastern__



__Harvard CS 229br description:__
This will be a graduate level course on recent advances and open questions in the theory of machine learning and specifically deep learning. We will review both classical results as well as recent papers in areas including classifiers and generalization gaps, representation learning, generative models, adversarial robustness and out of distribution performance, and more. 

This is a fast-moving area and it will be a fast-moving course. We will aim to cover both state-of-art results, as well as the intellectual foundations for them, and have a substantive discussion on both the “big picture” and technical details of the papers. In addition to the theoretical lectures, the course will involve a programming component aiming to get students to the point where they can both reproduce results from papers and work on their own research. This component will be largely self-directed and we expect students to be proficient in Python and in picking up technologies and libraries on their own (aka “Stack Overflow oriented programming”). We will ensure students have access to the appropriate computational resources (i.e., GPUs).

__MIT "Sister seminar":__ This Harvard seminar will be coordinated with a "sister seminar" at MIT, taught by Ankur Moitra. We recommend that students taking CS 229br also take the MIT course, but this is not required. The two courses will share some but not all lectures and assignments.  So, if you take CS 229br, please keep the Wednesday 12-3 slot free as well. 
 
__Prerequisites:__ The course will require mathematical maturity, and proficiency with proofs, probability, and information theory, as well as the basics of machine learning. We expect that students will have both theory background (at Harvard: CS 121 and CS 124 or similar, at MIT: 6.046 or similar) as well as machine learning background (at Harvard: CS 181 or 183 or similar, at MIT: 6.036 or similar). 

Students interested in this seminar will likely also be interested in following our [machine learning theory seminar series](https://mltheory.org/#talks). You can sign up for the mailing list to get announcements and Zoom links. Also, since last spring, all the talks in this series have been recorded and available on the webpage.



__Apply for both courses:__ Both Harvard CS 299br and MIT 18.400 will have a limited number of slots, you can apply to both the Harvard and MIT courses by [filling out this form](http://tiny.cc/mltheoryseminar). You can apply to one or both of the courses.


### Tentative list of topics (subject to change): 

__Harvard CS 229br:__

* Generalization bounds, and the extent that they are meaningful and/or proxies to actual performance.
* Out of distribution performance, Imagenet v2 and CIFAR v2, domain shift
* Unsupervised and self-supervised learning
* Transfer learning, meta learning
* Generative models (GANs,  VAE variants, DDPM, autoregressive models,...) 
* Natural language processing, language models
* Theoretical neuroscience, interpretation and visualization of neural networks
* Reinforcement learning, bandits
* Control theory
* Privacy, adversarial robustness, fairness/calibration
* Statistical physics view of machine learning.

__MIT 18.400:__

* Approximation theory: Neural networks as universal approximators, Fourier transform of Neural networks, Barron's Theorem, depth separation and open problems
* Optimization theory: Convex optimization and acceleration, escaping local minima.
* Solution landscape: Landscape analysis, escaping local minima, implicit regularization, Neural Tangent Kernel, implicit regularization, Langevin diffusion and meta-stability.
* Statistical theory and representation learning: capacity and generalization, VC, margin, and Radamacher complexity, capacity and generalization, student-teacher networks, sparse coding, restricted Boltzman machines.
* Sparse coding, spiked tensor models.
* Robustness : adversarial examples and defenses, data poisoning attacks.
 





